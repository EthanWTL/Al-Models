{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c80d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/Ethan/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e18cc37a0848b99d03e686e295e873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bcb38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610f01c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n",
    "print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n",
    "print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba90c771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Ethan\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-0c357a91ac1d3bad.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure only one answer for each question\n",
    "raw_datasets['train'].filter(lambda x: len(x['answers']['text']) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab32e92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Santa Clara, California',\n",
       "  \"Levi's Stadium\",\n",
       "  \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"],\n",
       " 'answer_start': [403, 355, 355]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can have more answers for the validationsets\n",
    "raw_datasets['validation'][2]['answers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be17c789",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed15860",
   "metadata": {},
   "source": [
    "### convert Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f78f9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15adae94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf52eb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trial tokenize the inputs for question and context\n",
    "context = raw_datasets[\"train\"][0][\"context\"]\n",
    "question = raw_datasets[\"train\"][0][\"question\"]\n",
    "\n",
    "inputs = tokenizer(question, context)\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015cee30",
   "metadata": {},
   "source": [
    "### since the text might be longer that the limit, we use sliding window to truncate the features, with length 100 and stride 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbacfa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length = 100,\n",
    "    truncation = \"only_second\",\n",
    "    stride = 50,\n",
    "    return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "for ids in inputs['input_ids']:\n",
    "    print(tokenizer.decode(ids))\n",
    "# we have four parts, for the parts without an answer, we will leave start only without end as 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36656033",
   "metadata": {},
   "source": [
    "### besides that, we also need to calculate out the end index using start + length of the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b91f03f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length = 100,\n",
    "    truncation = \"only_second\",\n",
    "    stride = 50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True, # this will calculate the end, base on the start and length of the answers\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2c18fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0),\n",
       "  (0, 2),\n",
       "  (3, 7),\n",
       "  (8, 11),\n",
       "  (12, 15),\n",
       "  (16, 22),\n",
       "  (23, 27),\n",
       "  (28, 37),\n",
       "  (38, 44),\n",
       "  (45, 47),\n",
       "  (48, 52),\n",
       "  (53, 55),\n",
       "  (56, 59),\n",
       "  (59, 63),\n",
       "  (64, 70),\n",
       "  (70, 71),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 2),\n",
       "  (3, 7),\n",
       "  (8, 11),\n",
       "  (12, 15),\n",
       "  (16, 22),\n",
       "  (23, 27),\n",
       "  (28, 37),\n",
       "  (38, 44),\n",
       "  (45, 47),\n",
       "  (48, 52),\n",
       "  (53, 55),\n",
       "  (56, 59),\n",
       "  (59, 63),\n",
       "  (64, 70),\n",
       "  (70, 71),\n",
       "  (0, 0),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 2),\n",
       "  (3, 7),\n",
       "  (8, 11),\n",
       "  (12, 15),\n",
       "  (16, 22),\n",
       "  (23, 27),\n",
       "  (28, 37),\n",
       "  (38, 44),\n",
       "  (45, 47),\n",
       "  (48, 52),\n",
       "  (53, 55),\n",
       "  (56, 59),\n",
       "  (59, 63),\n",
       "  (64, 70),\n",
       "  (70, 71),\n",
       "  (0, 0),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 2),\n",
       "  (3, 7),\n",
       "  (8, 11),\n",
       "  (12, 15),\n",
       "  (16, 22),\n",
       "  (23, 27),\n",
       "  (28, 37),\n",
       "  (38, 44),\n",
       "  (45, 47),\n",
       "  (48, 52),\n",
       "  (53, 55),\n",
       "  (56, 59),\n",
       "  (59, 63),\n",
       "  (64, 70),\n",
       "  (70, 71),\n",
       "  (0, 0),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23e013e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a1d14",
   "metadata": {},
   "source": [
    "### let's try more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e5f2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets['train'][2:6]['question'],\n",
    "    raw_datasets['train'][2:6]['context'],\n",
    "    max_length = 100,\n",
    "    truncation = \"only_second\",\n",
    "    stride = 50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True, # this will calculate the end, base on the start and length of the answers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c343471a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4 examples gave 19 features.\n",
      "Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].\n"
     ]
    }
   ],
   "source": [
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fdc4c1",
   "metadata": {},
   "source": [
    "## mapping the answer back to features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7ee51",
   "metadata": {},
   "source": [
    "## try with only 4 records first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "894ff538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0),\n",
       "  (0, 3),\n",
       "  (4, 12),\n",
       "  (13, 15),\n",
       "  (16, 19),\n",
       "  (20, 26),\n",
       "  (27, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 46),\n",
       "  (47, 49),\n",
       "  (50, 56),\n",
       "  (57, 59),\n",
       "  (60, 65),\n",
       "  (66, 75),\n",
       "  (75, 76),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 3),\n",
       "  (4, 12),\n",
       "  (13, 15),\n",
       "  (16, 19),\n",
       "  (20, 26),\n",
       "  (27, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 46),\n",
       "  (47, 49),\n",
       "  (50, 56),\n",
       "  (57, 59),\n",
       "  (60, 65),\n",
       "  (66, 75),\n",
       "  (75, 76),\n",
       "  (0, 0),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 3),\n",
       "  (4, 12),\n",
       "  (13, 15),\n",
       "  (16, 19),\n",
       "  (20, 26),\n",
       "  (27, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 46),\n",
       "  (47, 49),\n",
       "  (50, 56),\n",
       "  (57, 59),\n",
       "  (60, 65),\n",
       "  (66, 75),\n",
       "  (75, 76),\n",
       "  (0, 0),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 3),\n",
       "  (4, 12),\n",
       "  (13, 15),\n",
       "  (16, 19),\n",
       "  (20, 26),\n",
       "  (27, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 46),\n",
       "  (47, 49),\n",
       "  (50, 56),\n",
       "  (57, 59),\n",
       "  (60, 65),\n",
       "  (66, 75),\n",
       "  (75, 76),\n",
       "  (0, 0),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 11),\n",
       "  (12, 13),\n",
       "  (13, 16),\n",
       "  (16, 18),\n",
       "  (19, 21),\n",
       "  (22, 27),\n",
       "  (28, 32),\n",
       "  (32, 33),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 11),\n",
       "  (12, 13),\n",
       "  (13, 16),\n",
       "  (16, 18),\n",
       "  (19, 21),\n",
       "  (22, 27),\n",
       "  (28, 32),\n",
       "  (32, 33),\n",
       "  (0, 0),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 11),\n",
       "  (12, 13),\n",
       "  (13, 16),\n",
       "  (16, 18),\n",
       "  (19, 21),\n",
       "  (22, 27),\n",
       "  (28, 32),\n",
       "  (32, 33),\n",
       "  (0, 0),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 11),\n",
       "  (12, 13),\n",
       "  (13, 16),\n",
       "  (16, 18),\n",
       "  (19, 21),\n",
       "  (22, 27),\n",
       "  (28, 32),\n",
       "  (32, 33),\n",
       "  (0, 0),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 9),\n",
       "  (10, 12),\n",
       "  (13, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 28),\n",
       "  (29, 37),\n",
       "  (38, 40),\n",
       "  (41, 46),\n",
       "  (47, 51),\n",
       "  (51, 52),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 9),\n",
       "  (10, 12),\n",
       "  (13, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 28),\n",
       "  (29, 37),\n",
       "  (38, 40),\n",
       "  (41, 46),\n",
       "  (47, 51),\n",
       "  (51, 52),\n",
       "  (0, 0),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 9),\n",
       "  (10, 12),\n",
       "  (13, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 28),\n",
       "  (29, 37),\n",
       "  (38, 40),\n",
       "  (41, 46),\n",
       "  (47, 51),\n",
       "  (51, 52),\n",
       "  (0, 0),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 9),\n",
       "  (10, 12),\n",
       "  (13, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 28),\n",
       "  (29, 37),\n",
       "  (38, 40),\n",
       "  (41, 46),\n",
       "  (47, 51),\n",
       "  (51, 52),\n",
       "  (0, 0),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 8),\n",
       "  (9, 12),\n",
       "  (13, 15),\n",
       "  (15, 18),\n",
       "  (18, 23),\n",
       "  (24, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 45),\n",
       "  (45, 46),\n",
       "  (47, 52),\n",
       "  (53, 63),\n",
       "  (63, 64),\n",
       "  (0, 0),\n",
       "  (0, 2),\n",
       "  (3, 5),\n",
       "  (6, 10),\n",
       "  (11, 16),\n",
       "  (17, 29),\n",
       "  (29, 30),\n",
       "  (31, 36),\n",
       "  (37, 41),\n",
       "  (41, 42),\n",
       "  (42, 43),\n",
       "  (44, 52),\n",
       "  (53, 56),\n",
       "  (57, 58),\n",
       "  (59, 65),\n",
       "  (66, 68),\n",
       "  (69, 73),\n",
       "  (74, 79),\n",
       "  (80, 87),\n",
       "  (87, 88),\n",
       "  (89, 92),\n",
       "  (93, 97),\n",
       "  (98, 105),\n",
       "  (105, 106),\n",
       "  (106, 109),\n",
       "  (110, 117),\n",
       "  (118, 125),\n",
       "  (126, 131),\n",
       "  (132, 142),\n",
       "  (142, 143),\n",
       "  (144, 148),\n",
       "  (149, 150),\n",
       "  (151, 156),\n",
       "  (157, 160),\n",
       "  (161, 171),\n",
       "  (172, 179),\n",
       "  (179, 180),\n",
       "  (181, 184),\n",
       "  (185, 192),\n",
       "  (193, 202),\n",
       "  (203, 206),\n",
       "  (207, 215),\n",
       "  (215, 216),\n",
       "  (217, 219),\n",
       "  (219, 222),\n",
       "  (223, 225),\n",
       "  (226, 227),\n",
       "  (228, 231),\n",
       "  (231, 232),\n",
       "  (232, 236),\n",
       "  (237, 244),\n",
       "  (245, 247),\n",
       "  (248, 257),\n",
       "  (258, 262),\n",
       "  (262, 263),\n",
       "  (264, 267),\n",
       "  (268, 270),\n",
       "  (270, 273),\n",
       "  (273, 278),\n",
       "  (279, 287),\n",
       "  (288, 290),\n",
       "  (291, 297),\n",
       "  (298, 303),\n",
       "  (304, 311),\n",
       "  (312, 315),\n",
       "  (316, 322),\n",
       "  (323, 325),\n",
       "  (326, 328),\n",
       "  (329, 332),\n",
       "  (333, 339),\n",
       "  (340, 350),\n",
       "  (351, 361),\n",
       "  (362, 373),\n",
       "  (374, 376),\n",
       "  (377, 380),\n",
       "  (381, 387),\n",
       "  (388, 394),\n",
       "  (394, 395),\n",
       "  (396, 399),\n",
       "  (400, 405),\n",
       "  (406, 414),\n",
       "  (414, 415),\n",
       "  (416, 419),\n",
       "  (420, 422),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 8),\n",
       "  (9, 12),\n",
       "  (13, 15),\n",
       "  (15, 18),\n",
       "  (18, 23),\n",
       "  (24, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 45),\n",
       "  (45, 46),\n",
       "  (47, 52),\n",
       "  (53, 63),\n",
       "  (63, 64),\n",
       "  (0, 0),\n",
       "  (161, 171),\n",
       "  (172, 179),\n",
       "  (179, 180),\n",
       "  (181, 184),\n",
       "  (185, 192),\n",
       "  (193, 202),\n",
       "  (203, 206),\n",
       "  (207, 215),\n",
       "  (215, 216),\n",
       "  (217, 219),\n",
       "  (219, 222),\n",
       "  (223, 225),\n",
       "  (226, 227),\n",
       "  (228, 231),\n",
       "  (231, 232),\n",
       "  (232, 236),\n",
       "  (237, 244),\n",
       "  (245, 247),\n",
       "  (248, 257),\n",
       "  (258, 262),\n",
       "  (262, 263),\n",
       "  (264, 267),\n",
       "  (268, 270),\n",
       "  (270, 273),\n",
       "  (273, 278),\n",
       "  (279, 287),\n",
       "  (288, 290),\n",
       "  (291, 297),\n",
       "  (298, 303),\n",
       "  (304, 311),\n",
       "  (312, 315),\n",
       "  (316, 322),\n",
       "  (323, 325),\n",
       "  (326, 328),\n",
       "  (329, 332),\n",
       "  (333, 339),\n",
       "  (340, 350),\n",
       "  (351, 361),\n",
       "  (362, 373),\n",
       "  (374, 376),\n",
       "  (377, 380),\n",
       "  (381, 387),\n",
       "  (388, 394),\n",
       "  (394, 395),\n",
       "  (396, 399),\n",
       "  (400, 405),\n",
       "  (406, 414),\n",
       "  (414, 415),\n",
       "  (416, 419),\n",
       "  (420, 422),\n",
       "  (422, 426),\n",
       "  (426, 427),\n",
       "  (427, 428),\n",
       "  (429, 431),\n",
       "  (432, 440),\n",
       "  (441, 446),\n",
       "  (447, 448),\n",
       "  (449, 453),\n",
       "  (454, 457),\n",
       "  (458, 465),\n",
       "  (466, 468),\n",
       "  (469, 476),\n",
       "  (477, 487),\n",
       "  (488, 491),\n",
       "  (492, 499),\n",
       "  (499, 500),\n",
       "  (501, 504),\n",
       "  (505, 509),\n",
       "  (510, 514),\n",
       "  (514, 518),\n",
       "  (519, 521),\n",
       "  (522, 531),\n",
       "  (532, 540),\n",
       "  (540, 541),\n",
       "  (542, 545),\n",
       "  (546, 556),\n",
       "  (557, 561),\n",
       "  (562, 569),\n",
       "  (570, 581),\n",
       "  (582, 591),\n",
       "  (591, 592),\n",
       "  (593, 597),\n",
       "  (598, 601),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 8),\n",
       "  (9, 12),\n",
       "  (13, 15),\n",
       "  (15, 18),\n",
       "  (18, 23),\n",
       "  (24, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 45),\n",
       "  (45, 46),\n",
       "  (47, 52),\n",
       "  (53, 63),\n",
       "  (63, 64),\n",
       "  (0, 0),\n",
       "  (326, 328),\n",
       "  (329, 332),\n",
       "  (333, 339),\n",
       "  (340, 350),\n",
       "  (351, 361),\n",
       "  (362, 373),\n",
       "  (374, 376),\n",
       "  (377, 380),\n",
       "  (381, 387),\n",
       "  (388, 394),\n",
       "  (394, 395),\n",
       "  (396, 399),\n",
       "  (400, 405),\n",
       "  (406, 414),\n",
       "  (414, 415),\n",
       "  (416, 419),\n",
       "  (420, 422),\n",
       "  (422, 426),\n",
       "  (426, 427),\n",
       "  (427, 428),\n",
       "  (429, 431),\n",
       "  (432, 440),\n",
       "  (441, 446),\n",
       "  (447, 448),\n",
       "  (449, 453),\n",
       "  (454, 457),\n",
       "  (458, 465),\n",
       "  (466, 468),\n",
       "  (469, 476),\n",
       "  (477, 487),\n",
       "  (488, 491),\n",
       "  (492, 499),\n",
       "  (499, 500),\n",
       "  (501, 504),\n",
       "  (505, 509),\n",
       "  (510, 514),\n",
       "  (514, 518),\n",
       "  (519, 521),\n",
       "  (522, 531),\n",
       "  (532, 540),\n",
       "  (540, 541),\n",
       "  (542, 545),\n",
       "  (546, 556),\n",
       "  (557, 561),\n",
       "  (562, 569),\n",
       "  (570, 581),\n",
       "  (582, 591),\n",
       "  (591, 592),\n",
       "  (593, 597),\n",
       "  (598, 601),\n",
       "  (602, 610),\n",
       "  (611, 620),\n",
       "  (621, 626),\n",
       "  (627, 630),\n",
       "  (631, 637),\n",
       "  (638, 647),\n",
       "  (648, 658),\n",
       "  (659, 662),\n",
       "  (663, 668),\n",
       "  (669, 673),\n",
       "  (673, 674),\n",
       "  (675, 678),\n",
       "  (679, 686),\n",
       "  (687, 689),\n",
       "  (690, 698),\n",
       "  (699, 703),\n",
       "  (704, 708),\n",
       "  (709, 714),\n",
       "  (715, 719),\n",
       "  (720, 723),\n",
       "  (724, 729),\n",
       "  (730, 734),\n",
       "  (734, 735),\n",
       "  (735, 736),\n",
       "  (737, 744),\n",
       "  (744, 745),\n",
       "  (746, 752),\n",
       "  (753, 755),\n",
       "  (755, 758),\n",
       "  (758, 763),\n",
       "  (764, 767),\n",
       "  (768, 771),\n",
       "  (772, 776),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 8),\n",
       "  (9, 12),\n",
       "  (13, 15),\n",
       "  (15, 18),\n",
       "  (18, 23),\n",
       "  (24, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 45),\n",
       "  (45, 46),\n",
       "  (47, 52),\n",
       "  (53, 63),\n",
       "  (63, 64),\n",
       "  (0, 0),\n",
       "  (501, 504),\n",
       "  (505, 509),\n",
       "  (510, 514),\n",
       "  (514, 518),\n",
       "  (519, 521),\n",
       "  (522, 531),\n",
       "  (532, 540),\n",
       "  (540, 541),\n",
       "  (542, 545),\n",
       "  (546, 556),\n",
       "  (557, 561),\n",
       "  (562, 569),\n",
       "  (570, 581),\n",
       "  (582, 591),\n",
       "  (591, 592),\n",
       "  (593, 597),\n",
       "  (598, 601),\n",
       "  (602, 610),\n",
       "  (611, 620),\n",
       "  (621, 626),\n",
       "  (627, 630),\n",
       "  (631, 637),\n",
       "  (638, 647),\n",
       "  (648, 658),\n",
       "  (659, 662),\n",
       "  (663, 668),\n",
       "  (669, 673),\n",
       "  (673, 674),\n",
       "  (675, 678),\n",
       "  (679, 686),\n",
       "  (687, 689),\n",
       "  (690, 698),\n",
       "  (699, 703),\n",
       "  (704, 708),\n",
       "  (709, 714),\n",
       "  (715, 719),\n",
       "  (720, 723),\n",
       "  (724, 729),\n",
       "  (730, 734),\n",
       "  (734, 735),\n",
       "  (735, 736),\n",
       "  (737, 744),\n",
       "  (744, 745),\n",
       "  (746, 752),\n",
       "  (753, 755),\n",
       "  (755, 758),\n",
       "  (758, 763),\n",
       "  (764, 767),\n",
       "  (768, 771),\n",
       "  (772, 776),\n",
       "  (776, 777),\n",
       "  (778, 781),\n",
       "  (782, 790),\n",
       "  (791, 793),\n",
       "  (794, 796),\n",
       "  (797, 808),\n",
       "  (809, 820),\n",
       "  (821, 824),\n",
       "  (825, 829),\n",
       "  (830, 833),\n",
       "  (834, 838),\n",
       "  (839, 840),\n",
       "  (841, 848),\n",
       "  (849, 856),\n",
       "  (857, 859),\n",
       "  (860, 863),\n",
       "  (864, 873),\n",
       "  (874, 883),\n",
       "  (884, 888),\n",
       "  (889, 892),\n",
       "  (893, 903),\n",
       "  (903, 904),\n",
       "  (905, 907),\n",
       "  (908, 912),\n",
       "  (912, 913),\n",
       "  (914, 918),\n",
       "  (919, 923),\n",
       "  (924, 932),\n",
       "  (933, 941),\n",
       "  (942, 946),\n",
       "  (947, 950),\n",
       "  (951, 959),\n",
       "  (960, 965),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 8),\n",
       "  (9, 12),\n",
       "  (13, 15),\n",
       "  (15, 18),\n",
       "  (18, 23),\n",
       "  (24, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 45),\n",
       "  (45, 46),\n",
       "  (47, 52),\n",
       "  (53, 63),\n",
       "  (63, 64),\n",
       "  (0, 0),\n",
       "  (704, 708),\n",
       "  (709, 714),\n",
       "  (715, 719),\n",
       "  (720, 723),\n",
       "  (724, 729),\n",
       "  (730, 734),\n",
       "  (734, 735),\n",
       "  (735, 736),\n",
       "  (737, 744),\n",
       "  (744, 745),\n",
       "  (746, 752),\n",
       "  (753, 755),\n",
       "  (755, 758),\n",
       "  (758, 763),\n",
       "  (764, 767),\n",
       "  (768, 771),\n",
       "  (772, 776),\n",
       "  (776, 777),\n",
       "  (778, 781),\n",
       "  (782, 790),\n",
       "  (791, 793),\n",
       "  (794, 796),\n",
       "  (797, 808),\n",
       "  (809, 820),\n",
       "  (821, 824),\n",
       "  (825, 829),\n",
       "  (830, 833),\n",
       "  (834, 838),\n",
       "  (839, 840),\n",
       "  (841, 848),\n",
       "  (849, 856),\n",
       "  (857, 859),\n",
       "  (860, 863),\n",
       "  (864, 873),\n",
       "  (874, 883),\n",
       "  (884, 888),\n",
       "  (889, 892),\n",
       "  (893, 903),\n",
       "  (903, 904),\n",
       "  (905, 907),\n",
       "  (908, 912),\n",
       "  (912, 913),\n",
       "  (914, 918),\n",
       "  (919, 923),\n",
       "  (924, 932),\n",
       "  (933, 941),\n",
       "  (942, 946),\n",
       "  (947, 950),\n",
       "  (951, 959),\n",
       "  (960, 965),\n",
       "  (966, 968),\n",
       "  (969, 973),\n",
       "  (974, 975),\n",
       "  (976, 988),\n",
       "  (989, 993),\n",
       "  (993, 994),\n",
       "  (995, 996),\n",
       "  (997, 1004),\n",
       "  (1005, 1014),\n",
       "  (1014, 1015),\n",
       "  (1016, 1022),\n",
       "  (1023, 1028),\n",
       "  (1029, 1032),\n",
       "  (1033, 1042),\n",
       "  (1042, 1043),\n",
       "  (1044, 1052),\n",
       "  (1052, 1053),\n",
       "  (1054, 1056),\n",
       "  (1057, 1061),\n",
       "  (1061, 1062),\n",
       "  (1063, 1067),\n",
       "  (1068, 1073),\n",
       "  (1074, 1082),\n",
       "  (1083, 1091),\n",
       "  (1092, 1096),\n",
       "  (1097, 1100),\n",
       "  (1101, 1106),\n",
       "  (1107, 1113),\n",
       "  (1114, 1115),\n",
       "  (1116, 1123),\n",
       "  (1124, 1128),\n",
       "  (1128, 1129),\n",
       "  (1130, 1133),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 8),\n",
       "  (9, 12),\n",
       "  (13, 15),\n",
       "  (15, 18),\n",
       "  (18, 23),\n",
       "  (24, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 45),\n",
       "  (45, 46),\n",
       "  (47, 52),\n",
       "  (53, 63),\n",
       "  (63, 64),\n",
       "  (0, 0),\n",
       "  (864, 873),\n",
       "  (874, 883),\n",
       "  (884, 888),\n",
       "  (889, 892),\n",
       "  (893, 903),\n",
       "  (903, 904),\n",
       "  (905, 907),\n",
       "  (908, 912),\n",
       "  (912, 913),\n",
       "  (914, 918),\n",
       "  (919, 923),\n",
       "  (924, 932),\n",
       "  (933, 941),\n",
       "  (942, 946),\n",
       "  (947, 950),\n",
       "  (951, 959),\n",
       "  (960, 965),\n",
       "  (966, 968),\n",
       "  (969, 973),\n",
       "  (974, 975),\n",
       "  (976, 988),\n",
       "  (989, 993),\n",
       "  (993, 994),\n",
       "  (995, 996),\n",
       "  (997, 1004),\n",
       "  (1005, 1014),\n",
       "  (1014, 1015),\n",
       "  (1016, 1022),\n",
       "  (1023, 1028),\n",
       "  (1029, 1032),\n",
       "  (1033, 1042),\n",
       "  (1042, 1043),\n",
       "  (1044, 1052),\n",
       "  (1052, 1053),\n",
       "  (1054, 1056),\n",
       "  (1057, 1061),\n",
       "  (1061, 1062),\n",
       "  (1063, 1067),\n",
       "  (1068, 1073),\n",
       "  (1074, 1082),\n",
       "  (1083, 1091),\n",
       "  (1092, 1096),\n",
       "  (1097, 1100),\n",
       "  (1101, 1106),\n",
       "  (1107, 1113),\n",
       "  (1114, 1115),\n",
       "  (1116, 1123),\n",
       "  (1124, 1128),\n",
       "  (1128, 1129),\n",
       "  (1130, 1133),\n",
       "  (1134, 1146),\n",
       "  (1147, 1152),\n",
       "  (1153, 1158),\n",
       "  (1159, 1164),\n",
       "  (1165, 1169),\n",
       "  (1170, 1174),\n",
       "  (1175, 1185),\n",
       "  (1185, 1186),\n",
       "  (1187, 1194),\n",
       "  (1195, 1200),\n",
       "  (1201, 1203),\n",
       "  (1204, 1213),\n",
       "  (1214, 1216),\n",
       "  (1217, 1222),\n",
       "  (1223, 1225),\n",
       "  (1226, 1229),\n",
       "  (1230, 1238),\n",
       "  (1238, 1239),\n",
       "  (1240, 1247),\n",
       "  (1247, 1248),\n",
       "  (1249, 1252),\n",
       "  (1253, 1258),\n",
       "  (1259, 1262),\n",
       "  (1263, 1274),\n",
       "  (1275, 1277),\n",
       "  (1278, 1281),\n",
       "  (1282, 1290),\n",
       "  (1290, 1291),\n",
       "  (1292, 1299),\n",
       "  (1299, 1300),\n",
       "  (1301, 1303),\n",
       "  (1304, 1310),\n",
       "  (1311, 1315),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 8),\n",
       "  (9, 12),\n",
       "  (13, 15),\n",
       "  (15, 18),\n",
       "  (18, 23),\n",
       "  (24, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 45),\n",
       "  (45, 46),\n",
       "  (47, 52),\n",
       "  (53, 63),\n",
       "  (63, 64),\n",
       "  (0, 0),\n",
       "  (1052, 1053),\n",
       "  (1054, 1056),\n",
       "  (1057, 1061),\n",
       "  (1061, 1062),\n",
       "  (1063, 1067),\n",
       "  (1068, 1073),\n",
       "  (1074, 1082),\n",
       "  (1083, 1091),\n",
       "  (1092, 1096),\n",
       "  (1097, 1100),\n",
       "  (1101, 1106),\n",
       "  (1107, 1113),\n",
       "  (1114, 1115),\n",
       "  (1116, 1123),\n",
       "  (1124, 1128),\n",
       "  (1128, 1129),\n",
       "  (1130, 1133),\n",
       "  (1134, 1146),\n",
       "  (1147, 1152),\n",
       "  (1153, 1158),\n",
       "  (1159, 1164),\n",
       "  (1165, 1169),\n",
       "  (1170, 1174),\n",
       "  (1175, 1185),\n",
       "  (1185, 1186),\n",
       "  (1187, 1194),\n",
       "  (1195, 1200),\n",
       "  (1201, 1203),\n",
       "  (1204, 1213),\n",
       "  (1214, 1216),\n",
       "  (1217, 1222),\n",
       "  (1223, 1225),\n",
       "  (1226, 1229),\n",
       "  (1230, 1238),\n",
       "  (1238, 1239),\n",
       "  (1240, 1247),\n",
       "  (1247, 1248),\n",
       "  (1249, 1252),\n",
       "  (1253, 1258),\n",
       "  (1259, 1262),\n",
       "  (1263, 1274),\n",
       "  (1275, 1277),\n",
       "  (1278, 1281),\n",
       "  (1282, 1290),\n",
       "  (1290, 1291),\n",
       "  (1292, 1299),\n",
       "  (1299, 1300),\n",
       "  (1301, 1303),\n",
       "  (1304, 1310),\n",
       "  (1311, 1315),\n",
       "  (1316, 1318),\n",
       "  (1319, 1332),\n",
       "  (1333, 1340),\n",
       "  (1341, 1344),\n",
       "  (1345, 1354),\n",
       "  (1355, 1362),\n",
       "  (1363, 1371),\n",
       "  (1371, 1372),\n",
       "  (1373, 1379),\n",
       "  (1380, 1388),\n",
       "  (1388, 1389),\n",
       "  (1390, 1394),\n",
       "  (1395, 1398),\n",
       "  (1399, 1404),\n",
       "  (1404, 1405),\n",
       "  (0, 0)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a322b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " None,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_ids = inputs.sequence_ids(7)\n",
    "sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64d906c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n",
       " [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = raw_datasets['train'][2:6]['answers']\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs['offset_mapping']):\n",
    "    sample_idx = inputs['overflow_to_sample_mapping'][i] # which sample we are on, like the first record, second record\n",
    "    answer = answers[sample_idx] # get the answer repeatedly for that record. \n",
    "    start_char = answer['answer_start'][0] # the answer start from this position\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0]) # end with start + len\n",
    "    sequence_ids = inputs.sequence_ids(i) # get this record's token id? \n",
    "    \n",
    "    # get the \"relative\" postion of offset tuple's index based on the sequence id\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1 \n",
    "    context_start = idx # skip the questions part, jump into the context\n",
    "    \n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx -1 # find the end, +1,-1 is smart, since context will end with None\n",
    "    \n",
    "    # if the context starts after the answer's start, or end before the answer end, \n",
    "    # means the context doesn't include all the answers\n",
    "    if(offset[context_start][0] > start_char or offset[context_end][0] < end_char):\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx+=1 # we keep increase idx until either go beyond the end, or pass the start of the answer\n",
    "        start_positions.append(idx-1) # so it will looks like this [0,0,0,9,0,40,0,...]\n",
    "        \n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1 # we keep decrease until start or below the end position of the answer\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc2e4a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(inputs['overflow_to_sample_mapping'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0f7df",
   "metadata": {},
   "source": [
    "### double check with the decode answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c379bd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ['the Main Building'], 'answer_start': [279]},\n",
       " {'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]},\n",
       " {'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]},\n",
       " {'text': ['September 1876'], 'answer_start': [248]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89d4055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual answer : the Main Building, predicted answer: the Main Building\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "sample_idx = inputs['overflow_to_sample_mapping'][0]\n",
    "answer = answers[sample_idx]['text'][0] # the actual answer\n",
    "\n",
    "start = start_positions[i]\n",
    "end = end_positions[i]\n",
    "labeled_answer = tokenizer.decode(inputs['input_ids'][0][start : end + 1]) # the answer after decode from the tokenized inputs\n",
    "print(f\"actual answer : {answer}, predicted answer: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a02bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n",
    "print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed6c71",
   "metadata": {},
   "source": [
    "# the actual function for preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b11fb6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=384\n",
    "stride=128\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples['question']] # get the questions without redundant spaces\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop(\"offset_mapping\") # pop out the mapping for sliding windows\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\") # pop out the relation\n",
    "    answers = examples['answers']\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    # same as the previous explanation\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "964e0a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Ethan\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-65feacafeb7c2366.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = raw_datasets['train'].map(\n",
    "    preprocess_training_examples,\n",
    "    batched = True,\n",
    "    remove_columns = raw_datasets['train'].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89ae606f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 88729)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets['train']), len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388be535",
   "metadata": {},
   "source": [
    "### the actual function for preprocess the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "599c5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "    \n",
    "    #1. change idx to actually ids, 2. switch offset map to None for questions and padding\n",
    "    for i in range (len(inputs['input_ids'])): # i will be all sliding windows\n",
    "        sample_idx = sample_map[i] # again, get which original record we are working on\n",
    "        example_ids.append(examples['id'][sample_idx]) # switch index into id: (0,1,2,3) --> (id1,id1,id1,id2)\n",
    "        \n",
    "        sequence_ids = inputs.sequence_ids(i)# get \"this\" sliding window's sequence ids\n",
    "        offset = inputs['offset_mapping'][i] # get the offset table for question and context\\\n",
    "        inputs['offset_mapping'][i] = [ # we update the offset table, None for question, no change for context\n",
    "            o if sequence_ids[k] == 1 else None for k,o in enumerate(offset)\n",
    "        ]\n",
    "        \n",
    "    inputs['example_id'] = example_ids # adding a new columns for ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e3fa90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10570, 10822)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a19e8",
   "metadata": {},
   "source": [
    "# post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5548ca83",
   "metadata": {},
   "source": [
    "### test on small validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "925dbc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Ethan\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-297acf91a2624012.arrow\n"
     ]
    }
   ],
   "source": [
    "small_eval_set = raw_datasets['validation'].select(range(100))\n",
    "trained_checkpoint = 'distilbert-base-cased-distilled-squad'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets['validation'].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68f1fc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "067d1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#switch back to the old tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c0e9b",
   "metadata": {},
   "source": [
    "### a test run to get some simulated outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df1909b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "eval_set_for_model = eval_set.remove_columns(['example_id','offset_mapping']) # we dont need those columns?\n",
    "eval_set_for_model.set_format('torch')\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83d0dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5283e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.2607305 , -5.178325  , -5.270898  , -6.0858765 , -6.450713  ,\n",
       "       -6.2670946 , -5.314639  , -6.3032956 , -6.0470343 , -7.3009343 ,\n",
       "       -5.6778765 , -3.7498586 , -4.7882466 , -0.38573128, -4.3966126 ,\n",
       "       -1.9304973 , -5.377075  , -4.388612  , -2.539995  , -4.352418  ,\n",
       "       -4.3879633 , -4.332363  , -4.6790547 , -3.579806  , -2.306936  ,\n",
       "       -6.553044  , -2.7710211 , -0.872208  , -3.0604684 , -2.9521334 ,\n",
       "       -4.1954756 , -1.3516891 , -3.725062  , -4.703031  , -4.068803  ,\n",
       "       -0.8678059 , -3.6668837 , -1.8827084 ,  4.400484  ,  2.943778  ,\n",
       "       -0.7979491 , -1.3878831 , -0.6945383 ,  1.5780765 , -1.7355467 ,\n",
       "        0.52236897, 10.694437  ,  4.4599767 , -1.3703719 , -0.04971688,\n",
       "        2.0126412 , -2.6718123 , -2.2983904 , -1.5135095 ,  0.02073593,\n",
       "       -2.732355  , -0.02802771,  9.80368   ,  2.7017334 , -1.893289  ,\n",
       "       -6.2241755 , -3.2045462 , -3.9968174 , -4.2383127 , -3.5693395 ,\n",
       "       -2.502742  , -4.011329  , -5.996893  , -4.775757  , -2.7713678 ,\n",
       "       -2.5308406 , -4.8725395 , -6.4014845 , -4.8849335 , -5.1912107 ,\n",
       "       -3.1706054 , -4.8221264 , -6.6687946 , -1.9078944 , -5.5711308 ,\n",
       "       -5.5316515 , -2.5100658 , -6.9493    , -6.810874  , -5.107876  ,\n",
       "       -6.9907665 , -6.33069   , -4.7778153 , -6.5314326 , -6.4361086 ,\n",
       "       -6.0640087 , -7.300222  , -4.8981485 , -5.9093776 , -7.524845  ,\n",
       "       -4.094487  , -5.698613  , -6.275292  , -5.6613073 , -8.317497  ,\n",
       "       -5.9076905 , -3.763218  , -6.328804  , -6.3134217 , -7.754653  ,\n",
       "       -5.649818  , -5.8621526 , -6.998746  , -6.437409  , -6.0058303 ,\n",
       "       -5.0720987 , -6.420484  , -7.5737123 , -8.612469  , -7.1622615 ,\n",
       "       -6.1005583 , -8.316832  , -7.7184043 , -6.921631  , -8.037514  ,\n",
       "       -8.333999  , -8.621495  , -9.039058  , -6.726689  , -6.744296  ,\n",
       "       -8.502033  , -8.221222  , -7.740018  , -7.532273  , -8.814262  ,\n",
       "       -7.196786  , -7.27695   , -6.563022  , -7.938036  , -7.598213  ,\n",
       "       -8.104631  , -5.751569  , -7.180561  , -8.063911  , -7.3976774 ,\n",
       "       -7.885645  , -7.5272307 , -8.466509  , -7.167356  , -7.32186   ,\n",
       "       -7.958205  , -8.591034  , -8.625212  , -8.27425   , -8.625457  ,\n",
       "       -6.742632  , -5.148638  , -7.6580973 , -6.7458596 , -7.7155747 ,\n",
       "       -7.894371  , -8.117332  , -7.893765  , -8.500583  , -7.273346  ,\n",
       "       -7.4330363 , -8.293209  , -8.059217  , -8.292223  , -7.1756587 ,\n",
       "       -5.944449  , -7.243595  , -8.138486  , -8.145779  , -5.8426423 ,\n",
       "       -7.659709  , -4.7882204 , -9.490805  , -9.489101  , -9.480648  ,\n",
       "       -9.471792  , -9.47136   , -9.47678   , -9.498123  , -9.488591  ,\n",
       "       -9.512032  , -9.497135  , -9.501507  , -9.497562  , -9.511492  ,\n",
       "       -9.494003  , -9.478874  , -9.489488  , -9.503639  , -9.497576  ,\n",
       "       -9.481525  , -9.487917  , -9.492025  , -9.50886   , -9.50128   ,\n",
       "       -9.502687  , -9.502521  , -9.504171  , -9.528532  , -9.509514  ,\n",
       "       -9.506755  , -9.5066395 , -9.509529  , -9.508085  , -9.507988  ,\n",
       "       -9.506678  , -9.525377  , -9.51557   , -9.506173  , -9.500422  ,\n",
       "       -9.498646  , -9.507121  , -9.500015  , -9.504552  , -9.491712  ,\n",
       "       -9.516349  , -9.528473  , -9.513241  , -9.4883    , -9.523897  ,\n",
       "       -9.52254   , -9.520803  , -9.531368  , -9.508774  , -9.513155  ,\n",
       "       -9.506834  , -9.51314   , -9.485174  , -9.498626  , -9.476766  ,\n",
       "       -9.487992  , -9.52119   , -9.488397  , -9.487629  , -9.484707  ,\n",
       "       -9.516188  , -9.50773   , -9.484897  , -9.490901  , -9.489674  ,\n",
       "       -9.501001  , -9.51544   , -9.523233  , -9.513191  , -9.514931  ,\n",
       "       -9.47975   , -9.501222  , -9.51666   , -9.503565  , -9.501172  ,\n",
       "       -9.479919  , -9.468859  , -9.475201  , -9.491141  , -9.494028  ,\n",
       "       -9.485718  , -9.481192  , -9.501436  , -9.502008  , -9.493559  ,\n",
       "       -9.497606  , -9.500978  , -9.508734  , -9.505534  , -9.482164  ,\n",
       "       -9.471141  , -9.481523  , -9.490836  , -9.493652  , -9.46952   ,\n",
       "       -9.46379   , -9.494393  , -9.486972  , -9.48896   , -9.470968  ,\n",
       "       -9.4980135 , -9.486802  , -9.499592  , -9.492536  , -9.495203  ,\n",
       "       -9.52946   , -9.493635  , -9.507556  , -9.480069  , -9.485302  ,\n",
       "       -9.488803  , -9.481792  , -9.462948  , -9.482849  , -9.466454  ,\n",
       "       -9.473714  , -9.458752  , -9.472048  , -9.481611  , -9.468561  ,\n",
       "       -9.465526  , -9.476611  , -9.478315  , -9.471287  , -9.47006   ,\n",
       "       -9.450436  , -9.462075  , -9.47741   , -9.469491  , -9.441469  ,\n",
       "       -9.462511  , -9.454131  , -9.451046  , -9.452427  , -9.444376  ,\n",
       "       -9.450174  , -9.421057  , -9.434793  , -9.449849  , -9.452514  ,\n",
       "       -9.463892  , -9.4520645 , -9.456527  , -9.4608755 , -9.458342  ,\n",
       "       -9.458105  , -9.458444  , -9.47296   , -9.472687  , -9.49877   ,\n",
       "       -9.492165  , -9.485963  , -9.484571  , -9.529712  , -9.490872  ,\n",
       "       -9.509275  , -9.483416  , -9.490002  , -9.468959  , -9.491259  ,\n",
       "       -9.497527  , -9.50058   , -9.481471  , -9.471893  , -9.503663  ,\n",
       "       -9.512823  , -9.497616  , -9.495423  , -9.496603  , -9.516241  ,\n",
       "       -9.496533  , -9.487732  , -9.508049  , -9.500345  , -9.510822  ,\n",
       "       -9.49282   , -9.512426  , -9.518468  , -9.519093  , -9.539051  ,\n",
       "       -9.528627  , -9.53771   , -9.53554   , -9.532368  , -9.539339  ,\n",
       "       -9.567405  , -9.557684  , -9.55448   , -9.53665   , -9.548328  ,\n",
       "       -9.540268  , -9.522884  , -9.51638   , -9.517018  , -9.527035  ,\n",
       "       -9.541706  , -9.522745  , -9.513388  , -9.532746  , -9.538663  ,\n",
       "       -9.526039  , -9.521004  , -9.501883  , -9.505051  , -9.518797  ,\n",
       "       -9.521231  , -9.52433   , -9.518315  , -9.52876   ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43f1e8",
   "metadata": {},
   "source": [
    "### create id to index map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6b242e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature in enumerate(eval_set):\n",
    "    example_to_features[feature['example_id']].append(idx) # reset the index actually, id back to one on one for index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5864742b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'56be4db0acb8001400a502ec': [0],\n",
       "             '56be4db0acb8001400a502ed': [1],\n",
       "             '56be4db0acb8001400a502ee': [2],\n",
       "             '56be4db0acb8001400a502ef': [3],\n",
       "             '56be4db0acb8001400a502f0': [4],\n",
       "             '56be8e613aeaaa14008c90d1': [5],\n",
       "             '56be8e613aeaaa14008c90d2': [6],\n",
       "             '56be8e613aeaaa14008c90d3': [7],\n",
       "             '56bea9923aeaaa14008c91b9': [8],\n",
       "             '56bea9923aeaaa14008c91ba': [9],\n",
       "             '56bea9923aeaaa14008c91bb': [10],\n",
       "             '56beace93aeaaa14008c91df': [11],\n",
       "             '56beace93aeaaa14008c91e0': [12],\n",
       "             '56beace93aeaaa14008c91e1': [13],\n",
       "             '56beace93aeaaa14008c91e2': [14],\n",
       "             '56beace93aeaaa14008c91e3': [15],\n",
       "             '56bf10f43aeaaa14008c94fd': [16],\n",
       "             '56bf10f43aeaaa14008c94fe': [17],\n",
       "             '56bf10f43aeaaa14008c94ff': [18],\n",
       "             '56bf10f43aeaaa14008c9500': [19],\n",
       "             '56bf10f43aeaaa14008c9501': [20],\n",
       "             '56d20362e7d4791d009025e8': [21],\n",
       "             '56d20362e7d4791d009025e9': [22],\n",
       "             '56d20362e7d4791d009025ea': [23],\n",
       "             '56d20362e7d4791d009025eb': [24],\n",
       "             '56d600e31c85041400946eae': [25],\n",
       "             '56d600e31c85041400946eb0': [26],\n",
       "             '56d600e31c85041400946eb1': [27],\n",
       "             '56d9895ddc89441400fdb50e': [28],\n",
       "             '56d9895ddc89441400fdb510': [29],\n",
       "             '56be4e1facb8001400a502f6': [30],\n",
       "             '56be4e1facb8001400a502f9': [31],\n",
       "             '56be4e1facb8001400a502fa': [32],\n",
       "             '56beaa4a3aeaaa14008c91c2': [33],\n",
       "             '56beaa4a3aeaaa14008c91c3': [34],\n",
       "             '56bead5a3aeaaa14008c91e9': [35],\n",
       "             '56bead5a3aeaaa14008c91ea': [36],\n",
       "             '56bead5a3aeaaa14008c91eb': [37],\n",
       "             '56bead5a3aeaaa14008c91ec': [38],\n",
       "             '56bead5a3aeaaa14008c91ed': [39],\n",
       "             '56bf159b3aeaaa14008c9507': [40],\n",
       "             '56bf159b3aeaaa14008c9508': [41],\n",
       "             '56bf159b3aeaaa14008c9509': [42],\n",
       "             '56bf159b3aeaaa14008c950a': [43],\n",
       "             '56bf159b3aeaaa14008c950b': [44],\n",
       "             '56d2045de7d4791d009025f3': [45],\n",
       "             '56d2045de7d4791d009025f4': [46],\n",
       "             '56d2045de7d4791d009025f5': [47],\n",
       "             '56d2045de7d4791d009025f6': [48],\n",
       "             '56d6017d1c85041400946ebe': [49],\n",
       "             '56d6017d1c85041400946ec1': [50],\n",
       "             '56d6017d1c85041400946ec2': [51],\n",
       "             '56d98a59dc89441400fdb52a': [52],\n",
       "             '56d98a59dc89441400fdb52b': [53],\n",
       "             '56d98a59dc89441400fdb52e': [54],\n",
       "             '56be4eafacb8001400a50302': [55],\n",
       "             '56be4eafacb8001400a50303': [56],\n",
       "             '56be4eafacb8001400a50304': [57],\n",
       "             '56beab833aeaaa14008c91d2': [58],\n",
       "             '56beab833aeaaa14008c91d3': [59],\n",
       "             '56beab833aeaaa14008c91d4': [60],\n",
       "             '56beae423aeaaa14008c91f4': [61],\n",
       "             '56beae423aeaaa14008c91f5': [62],\n",
       "             '56beae423aeaaa14008c91f6': [63],\n",
       "             '56beae423aeaaa14008c91f7': [64],\n",
       "             '56bf17653aeaaa14008c9511': [65],\n",
       "             '56bf17653aeaaa14008c9513': [66],\n",
       "             '56bf17653aeaaa14008c9514': [67],\n",
       "             '56bf17653aeaaa14008c9515': [68],\n",
       "             '56d204ade7d4791d00902603': [69],\n",
       "             '56d204ade7d4791d00902604': [70],\n",
       "             '56d601e41c85041400946ece': [71],\n",
       "             '56d601e41c85041400946ecf': [72],\n",
       "             '56d601e41c85041400946ed0': [73],\n",
       "             '56d601e41c85041400946ed1': [74],\n",
       "             '56d601e41c85041400946ed2': [75],\n",
       "             '56d98b33dc89441400fdb53b': [76],\n",
       "             '56d98b33dc89441400fdb53c': [77],\n",
       "             '56d98b33dc89441400fdb53d': [78],\n",
       "             '56d98b33dc89441400fdb53e': [79],\n",
       "             '56be5333acb8001400a5030a': [80],\n",
       "             '56be5333acb8001400a5030b': [81],\n",
       "             '56be5333acb8001400a5030c': [82],\n",
       "             '56be5333acb8001400a5030d': [83],\n",
       "             '56be5333acb8001400a5030e': [84],\n",
       "             '56beaf5e3aeaaa14008c91fd': [85],\n",
       "             '56beaf5e3aeaaa14008c91fe': [86],\n",
       "             '56beaf5e3aeaaa14008c91ff': [87],\n",
       "             '56beaf5e3aeaaa14008c9200': [88],\n",
       "             '56beaf5e3aeaaa14008c9201': [89],\n",
       "             '56bf1ae93aeaaa14008c951b': [90],\n",
       "             '56bf1ae93aeaaa14008c951c': [91],\n",
       "             '56bf1ae93aeaaa14008c951e': [92],\n",
       "             '56bf1ae93aeaaa14008c951f': [93],\n",
       "             '56d2051ce7d4791d00902608': [94],\n",
       "             '56d2051ce7d4791d00902609': [95],\n",
       "             '56d2051ce7d4791d0090260a': [96],\n",
       "             '56d2051ce7d4791d0090260b': [97],\n",
       "             '56d602631c85041400946ed8': [98],\n",
       "             '56d602631c85041400946eda': [99]})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "209a59e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.694437"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits[0][46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ea0659b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " [0, 3],\n",
       " [4, 11],\n",
       " [12, 16],\n",
       " [17, 19],\n",
       " [20, 25],\n",
       " [26, 30],\n",
       " [31, 33],\n",
       " [34, 39],\n",
       " [40, 44],\n",
       " [45, 47],\n",
       " [48, 51],\n",
       " [52, 57],\n",
       " [58, 65],\n",
       " [65, 66],\n",
       " [67, 73],\n",
       " [74, 77],\n",
       " [78, 85],\n",
       " [86, 88],\n",
       " [89, 95],\n",
       " [95, 96],\n",
       " [96, 97],\n",
       " [98, 105],\n",
       " [105, 106],\n",
       " [107, 112],\n",
       " [113, 119],\n",
       " [120, 123],\n",
       " [124, 129],\n",
       " [130, 135],\n",
       " [136, 139],\n",
       " [140, 146],\n",
       " [147, 150],\n",
       " [151, 155],\n",
       " [156, 161],\n",
       " [162, 170],\n",
       " [170, 171],\n",
       " [171, 172],\n",
       " [173, 182],\n",
       " [183, 184],\n",
       " [185, 191],\n",
       " [192, 197],\n",
       " [198, 202],\n",
       " [203, 212],\n",
       " [213, 216],\n",
       " [217, 218],\n",
       " [219, 228],\n",
       " [228, 229],\n",
       " [230, 236],\n",
       " [237, 247],\n",
       " [248, 251],\n",
       " [252, 258],\n",
       " [259, 262],\n",
       " [263, 268],\n",
       " [269, 274],\n",
       " [275, 279],\n",
       " [280, 283],\n",
       " [283, 284],\n",
       " [285, 294],\n",
       " [295, 299],\n",
       " [300, 304],\n",
       " [305, 312],\n",
       " [312, 313],\n",
       " [314, 315],\n",
       " [315, 316],\n",
       " [317, 322],\n",
       " [322, 323],\n",
       " [324, 327],\n",
       " [328, 331],\n",
       " [332, 338],\n",
       " [339, 345],\n",
       " [345, 346],\n",
       " [346, 347],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set['offset_mapping'][70]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1faa9a7",
   "metadata": {},
   "source": [
    "### compute the index for the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9cb5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example['id'] # 56be4db0acb8001400a502ec\n",
    "    context = example['context'] # context\n",
    "    answers = []\n",
    "    \n",
    "    for feature_idx in example_to_features[example_id]:  # get which actual record we are in \n",
    "        start_logit = start_logits[feature_idx] # find the start index for the answer based on which record we are on\n",
    "        end_logit = end_logits[feature_idx]\n",
    "        offsets = eval_set['offset_mapping'][feature_idx] # get the particular offset table for the record\n",
    "        \n",
    "        start_indexes = np.argsort(start_logit)[-1: -n_best -1 : -1].tolist() # get the 20th largest possibility for start index\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() # get 2oth largest possible end index\n",
    "        \n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue # if start or end falles in None area, which is out bound, question, or padding area\n",
    "                if(\n",
    "                    end_index < start_index \n",
    "                    or \n",
    "                    end_index - start_index + 1 > max_answer_length\n",
    "                ): # if not valid or too long\n",
    "                    continue\n",
    "                \n",
    "                # the rest of the pairs should all be possible solutions\n",
    "                answers.append(\n",
    "                    {\n",
    "                        # parsing the answer\n",
    "                        \"text\":context[offsets[start_index][0]:offsets[end_index][1]], \n",
    "                        #score =  sum of possibility of start and end\n",
    "                        \"logit_score\":start_logit[start_index] + end_logit[end_index], \n",
    "                    }\n",
    "                )\n",
    "    best_answer = max(answers,key = lambda x : x['logit_score']) # get the best score answer\n",
    "    predicted_answers.append({\"id\":example_id,\"prediction_text\":best_answer['text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2826f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56be4db0acb8001400a502ed', 'prediction_text': 'Carolina Panthers'},\n",
       " {'id': '56be4db0acb8001400a502ee',\n",
       "  'prediction_text': \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\"},\n",
       " {'id': '56be4db0acb8001400a502ef', 'prediction_text': 'Carolina Panthers'},\n",
       " {'id': '56be4db0acb8001400a502f0', 'prediction_text': 'gold'},\n",
       " {'id': '56be8e613aeaaa14008c90d1', 'prediction_text': 'golden anniversary'},\n",
       " {'id': '56be8e613aeaaa14008c90d2', 'prediction_text': 'February 7, 2016'},\n",
       " {'id': '56be8e613aeaaa14008c90d3',\n",
       "  'prediction_text': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference'},\n",
       " {'id': '56bea9923aeaaa14008c91b9', 'prediction_text': 'golden anniversary'},\n",
       " {'id': '56bea9923aeaaa14008c91ba',\n",
       "  'prediction_text': 'American Football Conference'},\n",
       " {'id': '56bea9923aeaaa14008c91bb', 'prediction_text': 'February 7, 2016'},\n",
       " {'id': '56beace93aeaaa14008c91df', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56beace93aeaaa14008c91e0', 'prediction_text': \"Levi's Stadium\"},\n",
       " {'id': '56beace93aeaaa14008c91e1',\n",
       "  'prediction_text': 'Santa Clara, California'},\n",
       " {'id': '56beace93aeaaa14008c91e2', 'prediction_text': 'Super Bowl L'},\n",
       " {'id': '56beace93aeaaa14008c91e3', 'prediction_text': '2015'},\n",
       " {'id': '56bf10f43aeaaa14008c94fd', 'prediction_text': '2016'},\n",
       " {'id': '56bf10f43aeaaa14008c94fe',\n",
       "  'prediction_text': 'Santa Clara, California'},\n",
       " {'id': '56bf10f43aeaaa14008c94ff', 'prediction_text': \"Levi's Stadium\"},\n",
       " {'id': '56bf10f43aeaaa14008c9500', 'prediction_text': '2410'},\n",
       " {'id': '56bf10f43aeaaa14008c9501', 'prediction_text': 'February 7, 2016'},\n",
       " {'id': '56d20362e7d4791d009025e8', 'prediction_text': '2015'},\n",
       " {'id': '56d20362e7d4791d009025e9', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56d20362e7d4791d009025ea', 'prediction_text': 'Carolina Panthers'},\n",
       " {'id': '56d20362e7d4791d009025eb', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56d600e31c85041400946eae', 'prediction_text': '2015'},\n",
       " {'id': '56d600e31c85041400946eb0', 'prediction_text': 'Carolina Panthers'},\n",
       " {'id': '56d600e31c85041400946eb1', 'prediction_text': \"Levi's Stadium\"},\n",
       " {'id': '56d9895ddc89441400fdb50e', 'prediction_text': 'Super Bowl 50'},\n",
       " {'id': '56d9895ddc89441400fdb510', 'prediction_text': 'Denver Broncos'},\n",
       " {'id': '56be4e1facb8001400a502f6', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56be4e1facb8001400a502f9', 'prediction_text': 'eight'},\n",
       " {'id': '56be4e1facb8001400a502fa', 'prediction_text': '1995'},\n",
       " {'id': '56beaa4a3aeaaa14008c91c2', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56beaa4a3aeaaa14008c91c3', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56bead5a3aeaaa14008c91e9', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56bead5a3aeaaa14008c91ea', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56bead5a3aeaaa14008c91eb', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56bead5a3aeaaa14008c91ec', 'prediction_text': 'four'},\n",
       " {'id': '56bead5a3aeaaa14008c91ed', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56bf159b3aeaaa14008c9507', 'prediction_text': '151'},\n",
       " {'id': '56bf159b3aeaaa14008c9508', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56bf159b3aeaaa14008c9509', 'prediction_text': '151 record'},\n",
       " {'id': '56bf159b3aeaaa14008c950a', 'prediction_text': 'four'},\n",
       " {'id': '56bf159b3aeaaa14008c950b', 'prediction_text': 'New England Patriots'},\n",
       " {'id': '56d2045de7d4791d009025f3', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56d2045de7d4791d009025f4', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d2045de7d4791d009025f5', 'prediction_text': 'eight'},\n",
       " {'id': '56d2045de7d4791d009025f6', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d6017d1c85041400946ebe', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56d6017d1c85041400946ec1', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d6017d1c85041400946ec2', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d98a59dc89441400fdb52a', 'prediction_text': 'Cam Newton'},\n",
       " {'id': '56d98a59dc89441400fdb52b', 'prediction_text': 'Arizona Cardinals'},\n",
       " {'id': '56d98a59dc89441400fdb52e', 'prediction_text': '1995'},\n",
       " {'id': '56be4eafacb8001400a50302', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56be4eafacb8001400a50303', 'prediction_text': 'two'},\n",
       " {'id': '56be4eafacb8001400a50304', 'prediction_text': 'Broncos'},\n",
       " {'id': '56beab833aeaaa14008c91d2', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56beab833aeaaa14008c91d3', 'prediction_text': 'five'},\n",
       " {'id': '56beab833aeaaa14008c91d4', 'prediction_text': 'Newton'},\n",
       " {'id': '56beae423aeaaa14008c91f4', 'prediction_text': 'seven'},\n",
       " {'id': '56beae423aeaaa14008c91f5', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56beae423aeaaa14008c91f6', 'prediction_text': 'three'},\n",
       " {'id': '56beae423aeaaa14008c91f7', 'prediction_text': 'two'},\n",
       " {'id': '56bf17653aeaaa14008c9511', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56bf17653aeaaa14008c9513', 'prediction_text': 'linebacker'},\n",
       " {'id': '56bf17653aeaaa14008c9514', 'prediction_text': 'five'},\n",
       " {'id': '56bf17653aeaaa14008c9515', 'prediction_text': 'two'},\n",
       " {'id': '56d204ade7d4791d00902603', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56d204ade7d4791d00902604', 'prediction_text': 'five'},\n",
       " {'id': '56d601e41c85041400946ece', 'prediction_text': 'seven'},\n",
       " {'id': '56d601e41c85041400946ecf', 'prediction_text': 'three'},\n",
       " {'id': '56d601e41c85041400946ed0', 'prediction_text': 'a fumble'},\n",
       " {'id': '56d601e41c85041400946ed1', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56d601e41c85041400946ed2', 'prediction_text': 'linebacker'},\n",
       " {'id': '56d98b33dc89441400fdb53b', 'prediction_text': 'seven'},\n",
       " {'id': '56d98b33dc89441400fdb53c', 'prediction_text': 'three'},\n",
       " {'id': '56d98b33dc89441400fdb53d', 'prediction_text': 'Von Miller'},\n",
       " {'id': '56d98b33dc89441400fdb53e', 'prediction_text': 'five'},\n",
       " {'id': '56be5333acb8001400a5030a', 'prediction_text': 'CBS'},\n",
       " {'id': '56be5333acb8001400a5030b', 'prediction_text': '$5 million'},\n",
       " {'id': '56be5333acb8001400a5030c', 'prediction_text': 'Coldplay'},\n",
       " {'id': '56be5333acb8001400a5030d',\n",
       "  'prediction_text': 'Beyonc and Bruno Mars'},\n",
       " {'id': '56be5333acb8001400a5030e', 'prediction_text': 'Super Bowl 50'},\n",
       " {'id': '56beaf5e3aeaaa14008c91fd', 'prediction_text': 'CBS'},\n",
       " {'id': '56beaf5e3aeaaa14008c91fe', 'prediction_text': '$5 million'},\n",
       " {'id': '56beaf5e3aeaaa14008c91ff',\n",
       "  'prediction_text': 'Beyonc and Bruno Mars'},\n",
       " {'id': '56beaf5e3aeaaa14008c9200',\n",
       "  'prediction_text': 'Beyonc and Bruno Mars'},\n",
       " {'id': '56beaf5e3aeaaa14008c9201',\n",
       "  'prediction_text': 'Beyonc and Bruno Mars'},\n",
       " {'id': '56bf1ae93aeaaa14008c951b', 'prediction_text': 'CBS'},\n",
       " {'id': '56bf1ae93aeaaa14008c951c', 'prediction_text': '$5 million'},\n",
       " {'id': '56bf1ae93aeaaa14008c951e',\n",
       "  'prediction_text': 'Beyonc and Bruno Mars'},\n",
       " {'id': '56bf1ae93aeaaa14008c951f', 'prediction_text': 'third'},\n",
       " {'id': '56d2051ce7d4791d00902608', 'prediction_text': 'CBS'},\n",
       " {'id': '56d2051ce7d4791d00902609', 'prediction_text': '$5 million'},\n",
       " {'id': '56d2051ce7d4791d0090260a', 'prediction_text': 'Coldplay'},\n",
       " {'id': '56d2051ce7d4791d0090260b',\n",
       "  'prediction_text': 'Beyonc and Bruno Mars'},\n",
       " {'id': '56d602631c85041400946ed8', 'prediction_text': 'CBS'},\n",
       " {'id': '56d602631c85041400946eda',\n",
       "  'prediction_text': 'Coldplay with special guest performers Beyonc and Bruno Mars'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84b8e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('squad') # load squad metric for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b914af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_answers = [\n",
    "    {\"id\":ex['id'], \"answers\":ex['answers']} for ex in small_eval_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32428159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theoretical_answers[0]['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e4e9849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "# trial evaluate, making sure the data we processed in a reasonable way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678a61cd",
   "metadata": {},
   "source": [
    "# the actual compute_metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8556dd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03c53e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda3b15829714ed881863bce0298e665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad82f5",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee11a853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92b68cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to C:\\Users\\Ethan\\.huggingface\\token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1f47dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    evaluation_strategy='no',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b09e565",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N:\\Github\\AllAIAnalysis\\bert-finetuned-squad is already a clone of https://huggingface.co/EthanWTL/bert-finetuned-squad. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "Using cuda_amp half precision backend\n",
      "Loading model from bert-finetuned-squad\\checkpoint-11092.\n",
      "D:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 88729\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33276\n",
      "  Number of trainable parameters = 107721218\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 1\n",
      "  Continuing training from global step 11092\n",
      "  Will skip the first 1 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babcc35e102244ff9e237a207438c678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22185' max='33276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22185/33276 15:10 < 15:10, 12.17 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.776600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.742400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.765200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.757600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.727600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.716300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.739600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.725600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.741900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.727300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-finetuned-squad\\checkpoint-22184\n",
      "Configuration saved in bert-finetuned-squad\\checkpoint-22184\\config.json\n",
      "Model weights saved in bert-finetuned-squad\\checkpoint-22184\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-squad\\checkpoint-22184\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-squad\\checkpoint-22184\\special_tokens_map.json\n",
      "tokenizer config file saved in bert-finetuned-squad\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-squad\\special_tokens_map.json\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] : 'C:\\\\Users\\\\Ethan\\\\AppData\\\\Local\\\\Temp\\\\tmp3qi_sfyu\\\\lfs_progress'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:627\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 627\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] : 'C:\\\\Users\\\\Ethan\\\\AppData\\\\Local\\\\Temp\\\\tmp3qi_sfyu\\\\lfs_progress'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:805\u001b[0m, in \u001b[0;36mTemporaryDirectory._rmtree.<locals>.onerror\u001b[1;34m(func, path, exc_info)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 805\u001b[0m     \u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# PermissionError is raised on FreeBSD for directories\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] : 'C:\\\\Users\\\\Ethan\\\\AppData\\\\Local\\\\Temp\\\\tmp3qi_sfyu\\\\lfs_progress'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:1841\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1841\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   1845\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:2093\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2090\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2094\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:2237\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2234\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(rng_states, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrng_state_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mprocess_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_push_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;66;03m# Maybe delete some older checkpoints.\u001b[39;00m\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:3404\u001b[0m, in \u001b[0;36mTrainer._push_from_checkpoint\u001b[1;34m(self, checkpoint_folder)\u001b[0m\n\u001b[0;32m   3402\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3403\u001b[0m         commit_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining in progress, epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 3404\u001b[0m     _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush_in_progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   3406\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3407\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   3408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_strategy \u001b[38;5;241m==\u001b[39m HubStrategy\u001b[38;5;241m.\u001b[39mCHECKPOINT:\n\u001b[0;32m   3409\u001b[0m         \u001b[38;5;66;03m# Move back the checkpoint to its place\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\repository.py:1438\u001b[0m, in \u001b[0;36mRepository.push_to_hub\u001b[1;34m(self, commit_message, blocking, clean_ok, auto_lfs_prune)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_add(auto_lfs_track\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_commit(commit_message)\n\u001b[1;32m-> 1438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgit_push\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morigin \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_branch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\repository.py:1213\u001b[0m, in \u001b[0;36mRepository.git_push\u001b[1;34m(self, upstream, blocking, auto_lfs_prune)\u001b[0m\n\u001b[0;32m   1210\u001b[0m                 logger\u001b[38;5;241m.\u001b[39mwarning(stderr)\n\u001b[0;32m   1212\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m return_code:\n\u001b[1;32m-> 1213\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError(\n\u001b[0;32m   1214\u001b[0m                     return_code, process\u001b[38;5;241m.\u001b[39margs, output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr\n\u001b[0;32m   1215\u001b[0m                 )\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(exc\u001b[38;5;241m.\u001b[39mstderr)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\contextlib.py:126\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\repository.py:410\u001b[0m, in \u001b[0;36m_lfs_log_progress\u001b[1;34m()\u001b[0m\n\u001b[0;32m    407\u001b[0m exit_event\u001b[38;5;241m.\u001b[39mset()\n\u001b[0;32m    408\u001b[0m x\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m--> 410\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGIT_LFS_PROGRESS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_lfs_progress_value\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:830\u001b[0m, in \u001b[0;36mTemporaryDirectory.__exit__\u001b[1;34m(self, exc, value, tb)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc, value, tb):\n\u001b[1;32m--> 830\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:834\u001b[0m, in \u001b[0;36mTemporaryDirectory.cleanup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcleanup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalizer\u001b[38;5;241m.\u001b[39mdetach():\n\u001b[1;32m--> 834\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:816\u001b[0m, in \u001b[0;36mTemporaryDirectory._rmtree\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 816\u001b[0m \u001b[43m_shutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:629\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    627\u001b[0m             os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m             \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:808\u001b[0m, in \u001b[0;36mTemporaryDirectory._rmtree.<locals>.onerror\u001b[1;34m(func, path, exc_info)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# PermissionError is raised on FreeBSD for directories\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIsADirectoryError\u001b[39;00m, \u001b[38;5;167;01mPermissionError\u001b[39;00m):\n\u001b[1;32m--> 808\u001b[0m         \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:816\u001b[0m, in \u001b[0;36mTemporaryDirectory._rmtree\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 816\u001b[0m \u001b[43m_shutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:610\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    608\u001b[0m         entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m     entries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m entries:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:607\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rmtree_unsafe\u001b[39m(path, onerror):\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m scandir_it:\n\u001b[0;32m    608\u001b[0m             entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] : 'C:\\\\Users\\\\Ethan\\\\AppData\\\\Local\\\\Temp\\\\tmp3qi_sfyu\\\\lfs_progress'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = validation_dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38eb1ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N:\\Github\\AllAIAnalysis\\bert-finetuned-squad is already a clone of https://huggingface.co/EthanWTL/bert-finetuned-squad. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "Using cuda_amp half precision backend\n",
      "Loading model from bert-finetuned-squad\\checkpoint-22184.\n",
      "D:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 88729\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33276\n",
      "  Number of trainable parameters = 107721218\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 2\n",
      "  Continuing training from global step 22184\n",
      "  Will skip the first 2 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea23621c58e4b139b6baf204be93d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33277' max='33276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33276/33276 15:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.511100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.535800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.512200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.532100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.506700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.511400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.496900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.484700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.495300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.505700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1353' max='1353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1353/1353 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-finetuned-squad\\checkpoint-33276\n",
      "Configuration saved in bert-finetuned-squad\\checkpoint-33276\\config.json\n",
      "Model weights saved in bert-finetuned-squad\\checkpoint-33276\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-squad\\checkpoint-33276\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-squad\\checkpoint-33276\\special_tokens_map.json\n",
      "tokenizer config file saved in bert-finetuned-squad\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-squad\\special_tokens_map.json\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] : 'C:\\\\Users\\\\Ethan\\\\AppData\\\\Local\\\\Temp\\\\tmp8ylg083_\\\\lfs_progress'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:627\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 627\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] : 'C:\\\\Users\\\\Ethan\\\\AppData\\\\Local\\\\Temp\\\\tmp8ylg083_\\\\lfs_progress'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:805\u001b[0m, in \u001b[0;36mTemporaryDirectory._rmtree.<locals>.onerror\u001b[1;34m(func, path, exc_info)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 805\u001b[0m     \u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# PermissionError is raised on FreeBSD for directories\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] : 'C:\\\\Users\\\\Ethan\\\\AppData\\\\Local\\\\Temp\\\\tmp8ylg083_\\\\lfs_progress'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:1841\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1841\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   1845\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:2093\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2090\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2094\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:2237\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2234\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(rng_states, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrng_state_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mprocess_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_push_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;66;03m# Maybe delete some older checkpoints.\u001b[39;00m\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:3404\u001b[0m, in \u001b[0;36mTrainer._push_from_checkpoint\u001b[1;34m(self, checkpoint_folder)\u001b[0m\n\u001b[0;32m   3402\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3403\u001b[0m         commit_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining in progress, epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 3404\u001b[0m     _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush_in_progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   3406\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3407\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   3408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_strategy \u001b[38;5;241m==\u001b[39m HubStrategy\u001b[38;5;241m.\u001b[39mCHECKPOINT:\n\u001b[0;32m   3409\u001b[0m         \u001b[38;5;66;03m# Move back the checkpoint to its place\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\repository.py:1438\u001b[0m, in \u001b[0;36mRepository.push_to_hub\u001b[1;34m(self, commit_message, blocking, clean_ok, auto_lfs_prune)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_add(auto_lfs_track\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_commit(commit_message)\n\u001b[1;32m-> 1438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgit_push\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morigin \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_branch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\repository.py:1213\u001b[0m, in \u001b[0;36mRepository.git_push\u001b[1;34m(self, upstream, blocking, auto_lfs_prune)\u001b[0m\n\u001b[0;32m   1210\u001b[0m                 logger\u001b[38;5;241m.\u001b[39mwarning(stderr)\n\u001b[0;32m   1212\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m return_code:\n\u001b[1;32m-> 1213\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError(\n\u001b[0;32m   1214\u001b[0m                     return_code, process\u001b[38;5;241m.\u001b[39margs, output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr\n\u001b[0;32m   1215\u001b[0m                 )\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(exc\u001b[38;5;241m.\u001b[39mstderr)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\contextlib.py:126\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\repository.py:410\u001b[0m, in \u001b[0;36m_lfs_log_progress\u001b[1;34m()\u001b[0m\n\u001b[0;32m    407\u001b[0m exit_event\u001b[38;5;241m.\u001b[39mset()\n\u001b[0;32m    408\u001b[0m x\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m--> 410\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGIT_LFS_PROGRESS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_lfs_progress_value\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:830\u001b[0m, in \u001b[0;36mTemporaryDirectory.__exit__\u001b[1;34m(self, exc, value, tb)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc, value, tb):\n\u001b[1;32m--> 830\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:834\u001b[0m, in \u001b[0;36mTemporaryDirectory.cleanup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcleanup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalizer\u001b[38;5;241m.\u001b[39mdetach():\n\u001b[1;32m--> 834\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:816\u001b[0m, in \u001b[0;36mTemporaryDirectory._rmtree\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 816\u001b[0m \u001b[43m_shutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:629\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    627\u001b[0m             os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m             \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:808\u001b[0m, in \u001b[0;36mTemporaryDirectory._rmtree.<locals>.onerror\u001b[1;34m(func, path, exc_info)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# PermissionError is raised on FreeBSD for directories\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIsADirectoryError\u001b[39;00m, \u001b[38;5;167;01mPermissionError\u001b[39;00m):\n\u001b[1;32m--> 808\u001b[0m         \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\tempfile.py:816\u001b[0m, in \u001b[0;36mTemporaryDirectory._rmtree\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 816\u001b[0m \u001b[43m_shutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:610\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    608\u001b[0m         entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m     entries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m entries:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\torch\\lib\\shutil.py:607\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rmtree_unsafe\u001b[39m(path, onerror):\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m scandir_it:\n\u001b[0;32m    608\u001b[0m             entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] : 'C:\\\\Users\\\\Ethan\\\\AppData\\\\Local\\\\Temp\\\\tmp8ylg083_\\\\lfs_progress'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = validation_dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6670117",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0490f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10822\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d5b18e52bf4d40b09be6e4620cd248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 81.40964995269631, 'f1': 88.81301224703492}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions,_,_=trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions # prediction will return us the index for start and end\n",
    "compute_metrics(start_logits,end_logits, validation_dataset, raw_datasets['validation'])\n",
    "# base line is 80.8 and 88.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd0c13f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-finetuned-squad\n",
      "Configuration saved in bert-finetuned-squad\\config.json\n",
      "Model weights saved in bert-finetuned-squad\\pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-squad\\tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-squad\\special_tokens_map.json\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad', 'type': 'squad', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n",
      "To https://huggingface.co/EthanWTL/bert-finetuned-squad\n",
      "   dd1625b..395e1e0  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# push to hub to save our fine-tuned model\n",
    "trainer.push_to_hub(commit_message=\"Training complete\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05edab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20ad02b2",
   "metadata": {},
   "source": [
    "# using the fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67f07af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233743ac29684d139bd6adf9954a8ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/671 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ethan\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading configuration file config.json from cache at C:\\Users\\Ethan/.cache\\huggingface\\hub\\models--EthanWTL--bert-finetuned-squad\\snapshots\\395e1e0fa79d7e6aaa969b429a69fc53f8bf224b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"EthanWTL/bert-finetuned-squad\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\Ethan/.cache\\huggingface\\hub\\models--EthanWTL--bert-finetuned-squad\\snapshots\\395e1e0fa79d7e6aaa969b429a69fc53f8bf224b\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"EthanWTL/bert-finetuned-squad\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1823b5394fb5416494aa0d0c089fa87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Ethan/.cache\\huggingface\\hub\\models--EthanWTL--bert-finetuned-squad\\snapshots\\395e1e0fa79d7e6aaa969b429a69fc53f8bf224b\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at EthanWTL/bert-finetuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd0b65af1ad4b829d843604d81acd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003375b651b6449593cff4ad3861ef35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e19dfb5fe5c4468923c886fba873889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/669k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e69297512654b699735e9ff6d31ad5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\Ethan/.cache\\huggingface\\hub\\models--EthanWTL--bert-finetuned-squad\\snapshots\\395e1e0fa79d7e6aaa969b429a69fc53f8bf224b\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\Ethan/.cache\\huggingface\\hub\\models--EthanWTL--bert-finetuned-squad\\snapshots\\395e1e0fa79d7e6aaa969b429a69fc53f8bf224b\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Ethan/.cache\\huggingface\\hub\\models--EthanWTL--bert-finetuned-squad\\snapshots\\395e1e0fa79d7e6aaa969b429a69fc53f8bf224b\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Ethan/.cache\\huggingface\\hub\\models--EthanWTL--bert-finetuned-squad\\snapshots\\395e1e0fa79d7e6aaa969b429a69fc53f8bf224b\\tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9948900938034058,\n",
       " 'start': 78,\n",
       " 'end': 105,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = 'EthanWTL/bert-finetuned-squad'\n",
    "question_answerer = pipeline(\"question-answering\", model = model_checkpoint)\n",
    "\n",
    "context = \"\"\"\n",
    " Transformers is backed by the three most popular deep learning libraries  Jax, PyTorch and TensorFlow  with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back  Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da66dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49269d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c11343d",
   "metadata": {},
   "source": [
    "# customized Training Loop using Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7837bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    accelerator.print(\"Evaluation!\")\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(validation_dataset)]\n",
    "    end_logits = end_logits[: len(validation_dataset)]\n",
    "\n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"]\n",
    "    )\n",
    "    print(f\"epoch {epoch}:\", metrics)\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
